use online movie rating small dataset and send it to hadoop
// Isme when you copy the dataset from linux to hadoop...make sure that u delete the column names...
// if column names are not deleted then you will face an error "NumberFormatException"

steup project :

Open Eclipse> File > New > Java Project >( Name it) > Finish.
Right Click > New > Package ( Name it) > Finish.
Right Click on Package > New > Class (Name it).
Right Click on Project > Build Path> Add External
>>hadoop-Commons-cli-1.2.jar

after putting code 
export
1. Right Click on Project> Export> Select export destination as Jar File  > next> Finish.
2. import music dataset >>Movie >> ratingsmall



open terminal 
hadoop fs -put ratingsmall.csv wc
hadoop jar ps1_wcjar.jar pack.myclass wc dirw1
hadoop fs -ls dirw1
hadoop fs -cat dirw1/part-r-00000

open terminal 
hadoop fs -put wordcountFile.txt wc
hadoop jar ps1_wcjar.jar pack.myclass wc dirw1
hadoop fs -ls dirw1
hadoop fs -cat dirw1/part-r-00000

code: 
// Isme when you copy the dataset from linux to hadoop...make sure that u delete the column names...
// if column names are not deleted then you will face an error "NumberFormatException"

package pk1;

import java.io.IOException;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Job;

import org.apache.hadoop.util.GenericOptionsParser;

public class Movie {
	public static void main(String [] args) throws Exception{
		Configuration c = new Configuration();
		String[] files = new GenericOptionsParser(c, args).getRemainingArgs();
		
		Path input = new Path(files[0]);
		Path output = new Path(files[1]);
		
		Job j = new Job(c, "new job");
		j.setJarByClass(Movie.class);
		j.setMapperClass(MyMapper.class);
		j.setReducerClass(MyReducer.class);
		j.setOutputKeyClass(Text.class);
		j.setOutputValueClass(FloatWritable.class);
		FileInputFormat.addInputPath(j,input);
		FileOutputFormat.setOutputPath(j,output);
		
		System.exit(j.waitForCompletion(true)?0:1);
		
	}
	
	public static class MyMapper extends Mapper<LongWritable, Text, Text, FloatWritable>{
		public void map(LongWritable key, Text value, Context con) throws IOException, InterruptedException{
			String tmp = value.toString();
			String []lines = tmp.split("\n");
			for(String line: lines){
				String []eachline = line.split(",");
				float rating = Float.parseFloat(eachline[2]);
				con.write(new Text(eachline[1]), new FloatWritable(rating));
			}
		}
	}
	
	public static class  MyReducer extends Reducer<Text, FloatWritable, Text, FloatWritable>{
		public void reduce(Text key, Iterable<FloatWritable> vals, Context con) throws IOException, InterruptedException{
			float tot = 0;
			int cnt =0;
			for(FloatWritable v1: vals){
				tot += v1.get();
				cnt += 1;
			}
			float var = tot/(float)cnt;
			con.write(key, new FloatWritable(var));
		}
	}
	


}